{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition and Evaluation\n",
    "## Table of Contents\n",
    "1. [Model Selection](#model-selection)\n",
    "2. [Feature Engineering](#feature-engineering)\n",
    "3. [Hyperparameter Tuning](#hyperparameter-tuning)\n",
    "4. [Implementation](#implementation)\n",
    "5. [Evaluation Metrics](#evaluation-metrics)\n",
    "6. [Comparative Analysis](#comparative-analysis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, classification_report\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import keras_tuner as kt\n",
    "import matplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "[Discuss the type(s) of models you consider for this task, and justify the selection.]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "[Describe any additional feature engineering you've performed beyond what was done for the baseline model.]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "base_dir = os.path.dirname(os.getcwd())\n",
    "path_images = os.path.join(base_dir, \"1_DatasetCharacteristics\", \"EruptionImages\", \"sorted images\", \"train_val\")\n",
    "path_images_synth_input_act_yes = os.path.join(base_dir, \"1_DatasetCharacteristics\", \"EruptionImages\", \"sorted images\",\"train_val\", \"activity yes\")\n",
    "path_images_synth_input_act_no = os.path.join(base_dir, \"1_DatasetCharacteristics\", \"EruptionImages\", \"sorted images\" ,\"train_val\", \"activity no\")\n",
    "path_images_synth_output_act_yes = os.path.join(base_dir, \"1_DatasetCharacteristics\", \"EruptionImages\", \"synth\", \"synth activity yes\")\n",
    "path_images_synth_output_act_no = os.path.join(base_dir, \"1_DatasetCharacteristics\", \"EruptionImages\", \"synth\", \"synth activity no\")\n",
    "path_images_synth_output_input = os.path.join(base_dir, \"1_DatasetCharacteristics\", \"EruptionImages\", \"synth\")\n",
    "print(base_dir, path_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import original dataset\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    path_images,\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    label_mode='binary',\n",
    "    shuffle=True,   # randomize\n",
    "    color_mode='rgb',   # this strips alpha if present\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Get the class names\n",
    "print(\"Class names:\", dataset.class_names)\n",
    "\n",
    "counts = {0: 0, 1: 0}\n",
    "\n",
    "for images, labels in dataset:\n",
    "    unique, counts_batch = np.unique(labels.numpy(), return_counts=True)\n",
    "    for u, c in zip(unique, counts_batch):\n",
    "        counts[u] += c\n",
    "\n",
    "print(f\"Label counts: {counts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "# 1. Setup paths\n",
    "input_dir = path_images\n",
    "output_dir = path_images_synth\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 2. Define augmentation function\n",
    "def augment_image(image):\n",
    "    # Randomly rotate by 0°, 90°, 180°, or 270°\n",
    "    k = tf.random.uniform(shape=[], minval=0, maxval=4, dtype=tf.int32)\n",
    "    image = tf.image.rot90(image, k)\n",
    "\n",
    "    # Random brightness and contrast\n",
    "    image = tf.image.random_brightness(image, max_delta=0.2)\n",
    "    image = tf.image.random_contrast(image, lower=0.8, upper=1.2)\n",
    "\n",
    "    image = tf.clip_by_value(image, 0, 255)\n",
    "    return image\n",
    "\n",
    "# 3. Process each image\n",
    "for filename in os.listdir(input_dir):\n",
    "    if filename.endswith('.jpg') or filename.endswith('.png'):\n",
    "        img_path = os.path.join(input_dir, filename)\n",
    "        img_raw = tf.io.read_file(img_path)\n",
    "        img = tf.image.decode_image(img_raw, channels=3)\n",
    "        img = tf.cast(img, tf.float32)\n",
    "\n",
    "        for i in range(5):\n",
    "            aug_img = augment_image(img)\n",
    "            aug_img = tf.cast(aug_img, tf.uint8)\n",
    "\n",
    "            new_filename = f\"{os.path.splitext(filename)[0]}_aug{i}.jpg\"\n",
    "            output_path = os.path.join(output_dir, new_filename)\n",
    "\n",
    "            encoded_img = tf.io.encode_jpeg(aug_img)\n",
    "            tf.io.write_file(output_path, encoded_img)\n",
    "\n",
    "print(f\"Done. Augmented images saved to: {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_batches = tf.data.experimental.cardinality(dataset).numpy()\n",
    "train_batches = int(total_batches * 0.8)\n",
    "val_batches = total_batches - train_batches\n",
    "\n",
    "train_ds_baseline = dataset.take(train_batches)\n",
    "val_ds_baseline = dataset.skip(train_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### synthetic data: run once to create the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create synthetic data (run only onces)\n",
    "\n",
    "# paths = [\n",
    "#     (path_images_synth_input_act_yes, path_images_synth_output_act_yes),\n",
    "#     (path_images_synth_input_act_no, path_images_synth_output_act_no)\n",
    "# ]\n",
    "\n",
    "# # make sure output dirs exist\n",
    "# for _, output_dir in paths:\n",
    "#     os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# # augmentation\n",
    "# def augment_image(image):\n",
    "#     k = tf.random.uniform(shape=[], minval=0, maxval=4, dtype=tf.int32)\n",
    "#     image = tf.image.rot90(image, k)\n",
    "\n",
    "#     image = tf.image.random_brightness(image, max_delta=0.3)\n",
    "#     image = tf.image.random_contrast(image, lower=0.7, upper=1.3)\n",
    "\n",
    "#     noise = tf.random.normal(shape=tf.shape(image), mean=0.0, stddev=5.0)\n",
    "#     image = image + noise\n",
    "\n",
    "#     # simple blur\n",
    "#     kernel = tf.constant([[1/9, 1/9, 1/9],\n",
    "#                           [1/9, 1/9, 1/9],\n",
    "#                           [1/9, 1/9, 1/9]], dtype=tf.float32)\n",
    "#     kernel = kernel[:, :, tf.newaxis, tf.newaxis]\n",
    "#     kernel = tf.repeat(kernel, repeats=3, axis=2)\n",
    "\n",
    "#     image_exp = tf.expand_dims(image, axis=0)\n",
    "#     blurred = tf.nn.depthwise_conv2d(image_exp, kernel, strides=[1,1,1,1], padding='SAME')\n",
    "#     image = tf.squeeze(blurred, axis=0)\n",
    "\n",
    "#     image = tf.clip_by_value(image, 0, 255)\n",
    "#     return image\n",
    "\n",
    "# # loop over all (input, output) pairs\n",
    "# for input_dir, output_dir in paths:\n",
    "#     for filename in os.listdir(input_dir):\n",
    "#         if filename.lower().endswith('.jpg') or filename.lower().endswith('.png'):\n",
    "#             img_path = os.path.join(input_dir, filename)\n",
    "#             img_raw = tf.io.read_file(img_path)\n",
    "#             img = tf.image.decode_image(img_raw, channels=3)\n",
    "#             img = tf.cast(img, tf.float32)\n",
    "\n",
    "#             for i in range(5):\n",
    "#                 aug_img = augment_image(img)\n",
    "#                 aug_img = tf.cast(aug_img, tf.uint8)\n",
    "\n",
    "#                 new_filename = f\"{os.path.splitext(filename)[0]}_aug{i}.jpg\"\n",
    "#                 output_path = os.path.join(output_dir, new_filename)\n",
    "\n",
    "#                 encoded_img = tf.io.encode_jpeg(aug_img)\n",
    "#                 tf.io.write_file(output_path, encoded_img)\n",
    "\n",
    "# print(\"Done. All augmented images saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import test images with for the model unknown vulcano\n",
    "\n",
    "path_images_test = os.path.join(base_dir, \"1_DatasetCharacteristics\", \"EruptionImages\", \"sorted images\", \"test\")\n",
    "\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "TEST_DIR = path_images_test\n",
    "\n",
    "test_df = tf.keras.utils.image_dataset_from_directory(\n",
    "    TEST_DIR,\n",
    "    labels='inferred',\n",
    "    label_mode='binary',\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False \n",
    ")\n",
    "\n",
    "print(\"Test dataset loaded\")\n",
    "print(f\"Class names: {test_df.class_names}\")\n",
    "\n",
    "counts = {0: 0, 1: 0}\n",
    "\n",
    "for images, labels in test_df:\n",
    "    unique, counts_batch = np.unique(labels.numpy(), return_counts=True)\n",
    "    for u, c in zip(unique, counts_batch):\n",
    "        counts[u] += c\n",
    "\n",
    "print(f\"Label counts: {counts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import created synthetic images\n",
    "# only run if dataset was created with the algorithm above\n",
    "\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "dataset_synth = tf.keras.utils.image_dataset_from_directory(\n",
    "    path_images_synth_output_input,\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    label_mode='binary',\n",
    "    shuffle=True,   # randomize\n",
    "    color_mode='rgb',   # this strips alpha if present\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Get the class names\n",
    "print(\"Class names:\", dataset_synth.class_names)\n",
    "\n",
    "counts = {0: 0, 1: 0}\n",
    "\n",
    "for images, labels in dataset_synth:\n",
    "    unique, counts_batch = np.unique(labels.numpy(), return_counts=True)\n",
    "    for u, c in zip(unique, counts_batch):\n",
    "        counts[u] += c\n",
    "\n",
    "print(f\"Label counts: {counts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge both the original and the synth data together\n",
    "combined_dataset = dataset.concatenate(dataset_synth)\n",
    "\n",
    "# shuffle again for better mixing\n",
    "combined_dataset = combined_dataset.shuffle(1000)\n",
    "\n",
    "# prefetch for performance\n",
    "combined_dataset = combined_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "# check one batch\n",
    "for images, labels in combined_dataset.take(1):\n",
    "    print(\"Batch shape:\", images.shape, \"Labels:\", labels.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = {0: 0, 1: 0}\n",
    "\n",
    "for images, labels in combined_dataset:\n",
    "    unique, counts_batch = np.unique(labels.numpy(), return_counts=True)\n",
    "    for u, c in zip(unique, counts_batch):\n",
    "        counts[u] += c\n",
    "\n",
    "print(f\"Label counts: {counts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train and validation set \n",
    "\n",
    "total_batches = tf.data.experimental.cardinality(combined_dataset).numpy()\n",
    "train_batches = int(total_batches * 0.8)\n",
    "val_batches = total_batches - train_batches\n",
    "\n",
    "train_ds = combined_dataset.take(train_batches)\n",
    "val_ds = combined_dataset.skip(train_batches)\n",
    "\n",
    "def count_labels(ds):\n",
    "    counts = {0: 0, 1: 0}\n",
    "    for _, labels in ds:\n",
    "        unique, counts_batch = np.unique(labels.numpy(), return_counts=True)\n",
    "        for u, c in zip(unique, counts_batch):\n",
    "            counts[int(u)] += c\n",
    "    return counts\n",
    "\n",
    "print(\"Train:\", count_labels(train_ds))\n",
    "print(\"Validation:\", count_labels(val_ds))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "baseline_model = keras.Sequential([\n",
    "    layers.Rescaling(1./255, input_shape=(224, 224, 3)),\n",
    "\n",
    "    layers.Conv2D(8, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "\n",
    "    layers.Flatten(),\n",
    "\n",
    "    layers.Dense(8, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "\n",
    "baseline_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "baseline_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = baseline_model.fit(\n",
    "    train_ds_baseline,             \n",
    "    validation_data=val_ds_baseline,  \n",
    "    epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# extract metrics\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(1, len(acc) + 1)\n",
    "\n",
    "# plot\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.title('Training & Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.title('Training & Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for images, labels in test_df:\n",
    "    preds = baseline_model.predict(images)\n",
    "    y_true.extend(labels.numpy().flatten())\n",
    "    y_pred.extend(preds.flatten())\n",
    "\n",
    "# Convert probabilities to binary\n",
    "y_pred_labels = [1 if p >= 0.5 else 0 for p in y_pred]\n",
    "\n",
    "# Compute metrics\n",
    "acc = accuracy_score(y_true, y_pred_labels)\n",
    "prec = precision_score(y_true, y_pred_labels)\n",
    "rec = recall_score(y_true, y_pred_labels)\n",
    "f1 = f1_score(y_true, y_pred_labels)\n",
    "\n",
    "print(\"Accuracy:\", acc)\n",
    "print(\"Precision:\", prec)\n",
    "print(\"Recall:\", rec)\n",
    "print(\"F1 Score:\", f1)\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred_labels)\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model for synth dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Model 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Example model\n",
    "model_synth = keras.Sequential([\n",
    "    layers.Rescaling(1./255, input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3)),\n",
    "\n",
    "    layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D(),\n",
    "\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D(),\n",
    "\n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D(),\n",
    "\n",
    "    layers.Flatten(),\n",
    "\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "\n",
    "    layers.Dense(1, activation='sigmoid')  # binary classification\n",
    "])\n",
    "\n",
    "model_synth.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model_synth.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement early stopping:\n",
    "early_stop = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=2,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "    'best_model.keras',\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True\n",
    ")\n",
    "\n",
    "history_synth = model_synth.fit(\n",
    "    combined_dataset,\n",
    "    validation_data=test_df,\n",
    "    epochs=20,\n",
    "    callbacks=[early_stop, checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# extract metrics\n",
    "acc = history_synth.history['accuracy']\n",
    "val_acc = history_synth.history['val_accuracy']\n",
    "\n",
    "loss = history_synth.history['loss']\n",
    "val_loss = history_synth.history['val_loss']\n",
    "\n",
    "epochs_range = range(1, len(acc) + 1)\n",
    "\n",
    "# plot\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.title('Training & Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.title('Training & Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics\n",
    "\n",
    "To evaluate the performance of the VolcaNet model, we use the following metrics:\n",
    "\n",
    "Accuracy\n",
    "Measures the overall proportion of correctly classified images. While useful as a general indicator, accuracy alone may be misleading due to potential class imbalance (e.g., fewer eruption images compared to non-eruptions).\n",
    "\n",
    "Precision\n",
    "Precision for the “active” class tells us how many of the predicted volcanic activity events were actually correct. This is especially important when false alarms (false positives) could waste resources or trigger unnecessary alerts.\n",
    "\n",
    "Recall (Sensitivity)\n",
    "Recall indicates how many of the actual volcanic activity events were correctly identified by the model. It is critical for early warning systems, where missing a real eruption (false negative) could have serious consequences.\n",
    "\n",
    "F1-Score\n",
    "The harmonic mean of precision and recall. It provides a balanced measure when dealing with uneven class distributions and is useful for assessing the model's ability to identify active volcanic phases accurately and reliably.\n",
    "\n",
    "Confusion Matrix\n",
    "A visual representation of true vs. predicted labels for both classes. This helps identify specific failure modes (e.g., frequent false positives for certain volcanoes) and supports targeted model improvement.\n",
    "\n",
    "Why these metrics?\n",
    "Volcanic activity classification is a high-risk binary classification problem where both false negatives (missed eruptions) and false positives (false alerts) have significant implications. Therefore, metrics like recall, precision, and F1-score are more informative than accuracy alone. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('best_model.keras')\n",
    "# get true labels and predictions\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for images, labels in test_df:\n",
    "    preds = model_synth.predict(images)\n",
    "    y_true.extend(labels.numpy().flatten())\n",
    "    y_pred.extend(preds.flatten())\n",
    "\n",
    "# convert predicted probabilities to binary class\n",
    "y_pred_labels = [1 if p >= 0.5 else 0 for p in y_pred]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred_labels)\n",
    "precision = precision_score(y_true, y_pred_labels)\n",
    "recall = recall_score(y_true, y_pred_labels)\n",
    "f1 = f1_score(y_true, y_pred_labels)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred_labels)\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "In the MobileNetV2 notebook **[grid_search](3_Model\\grid_search_mobilenetv2_data_augmentation.ipynb)**, a grid search method was employed to systematically explore the effects of key hyperparameters on model performance. The parameters tuned included dropout rates (0.3 and 0.5), L2 regularization strength (0.001), learning rates (0.001 and 0.0005), and the number of training epochs (5 and 10). This approach was chosen to balance the model's ability to generalize while mitigating overfitting risks.\n",
    "\n",
    "Training was conducted in two phases: initially, the pre-trained MobileNetV2 base was frozen to leverage existing learned features, followed by fine-tuning of the last 30 layers with a reduced learning rate. This staged training strategy helps preserve useful generic representations while allowing the model to adapt to the specific dataset.\n",
    "\n",
    "Models were trained and evaluated for each combination of hyperparameters, with validation accuracies recorded to identify the best-performing configuration. \n",
    "\n",
    "\n",
    "**Implementation**\n",
    "The final model utilizes the MobileNetV2 architecture pre-trained on ImageNet as the base. Data augmentation techniques—including random flipping, rotation, zoom, contrast, and translation—are applied to the input images to improve generalization.\n",
    "\n",
    "Following the initial training phase with the base model frozen, the last 30 layers are unfrozen to enable fine-tuning with a lower learning rate. The model architecture consists of a global average pooling layer, followed by dropout and an L2-regularized dense layer with sigmoid activation for binary classification.\n",
    "\n",
    "Hyperparameters selected for the final model correspond to those achieving the highest validation accuracy during the grid search. The model is compiled using the Adam optimizer and binary cross-entropy loss, trained on the training dataset, validated on the validation dataset, and saved for further evaluation or deployment.\n",
    "\n",
    "For evaluation, the best model is loaded and tested on a separate test dataset. Predictions are generated and compared with true labels to compute performance metrics including accuracy, precision, recall, and F1 score. A confusion matrix is also visualized using a heatmap to provide insight into classification performance across classes. This comprehensive evaluation ensures the final model's effectiveness on unseen data. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparative Analysis\n",
    "\n",
    "**Baseline CNN** – Underperforming Reference Point\n",
    "The baseline model, a shallow one-layer CNN trained on the original dataset (830 images), performed poorly:\n",
    "\n",
    "Accuracy: 56.8%\n",
    "\n",
    "Precision: 78.6%\n",
    "\n",
    "Recall: 18.6%\n",
    "\n",
    "F1 Score: 0.30\n",
    "\n",
    "Despite decent precision, the recall was very low — meaning the model missed most actual eruptions. This renders it unsuitable for real-world monitoring where failing to detect true positives is unacceptable. Its limited depth and insufficient training data contributed to severe underfitting.\n",
    "\n",
    "**Custom Deep CNN** – Significant Improvement\n",
    "After data augmentation (4150 images) and architectural enhancements, the deeper CNN achieved much better results:\n",
    "\n",
    "Accuracy: 84.7%\n",
    "\n",
    "Precision: 1.00\n",
    "\n",
    "Recall: 69.5%\n",
    "\n",
    "F1 Score: 0.82\n",
    "\n",
    "This model managed to detect the majority of eruptions without producing any false positives. It represented a strong balance between robustness and simplicity, proving that careful architectural tuning and data expansion already bring significant gains over the baseline. However, some eruptions were still missed (false negatives), limiting its use in critical alerting scenarios.\n",
    "\n",
    "**MobileNetV** (Transfer Learning) – Best Overall Performer\n",
    "After implying the augmentation of the images in tensorflow directly in tensorflow and in combination with a pretrained MobileNetV2, we achieved state-of-the-art results:\n",
    "\n",
    "Accuracy: 96.6%\n",
    "\n",
    "Precision: 1.00\n",
    "\n",
    "Recall: 93.2%\n",
    "\n",
    "F1 Score: 0.965\n",
    "\n",
    "This model generalized extremely well. It caught nearly all eruption events and made no false predictions. The combination of transfer learning, regularization (L2), and dropout contributed to both stability and generalization. It clearly demonstrates the power of pre-trained networks, especially when training data is limited.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
